{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Q1\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.utils import resample\n",
        "from sklearn.feature_extraction import text as sk_text\n",
        "\n",
        "path_to_csv = \"spam.csv\"\n",
        "df = pd.read_csv(path_to_csv, encoding='latin-1')\n",
        "\n",
        "if 'v1' in df.columns and 'v2' in df.columns:\n",
        "    df = df.rename(columns={'v1':'label','v2':'text'})[['label','text']]\n",
        "elif 'label' in df.columns and 'text' in df.columns:\n",
        "    df = df[['label','text']]\n",
        "else:\n",
        "\n",
        "    cols = df.columns.tolist()\n",
        "    df = df[[cols[0], cols[1]]]\n",
        "    df.columns = ['label','text']\n",
        "\n",
        "\n",
        "df['label'] = df['label'].map({'spam':1,'ham':0})\n",
        "\n",
        "\n",
        "stopwords = sk_text.ENGLISH_STOP_WORDS\n",
        "def clean_text(s):\n",
        "    s = str(s).lower()\n",
        "    s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n",
        "    tokens = [w for w in s.split() if w not in stopwords]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['text_clean'] = df['text'].apply(clean_text)\n",
        "\n",
        "\n",
        "tfv = TfidfVectorizer(max_features=5000)\n",
        "X = tfv.fit_transform(df['text_clean'])\n",
        "y = df['label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "print(\"Train class dist:\", np.bincount(y_train))\n",
        "print(\"Test class dist :\", np.bincount(y_test))\n",
        "\n",
        "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "stump.fit(X_train, y_train)\n",
        "y_pred_train = stump.predict(X_train); y_pred_test = stump.predict(X_test)\n",
        "print(\"Stump Train Acc:\", accuracy_score(y_train, y_pred_train))\n",
        "print(\"Stump Test Acc :\", accuracy_score(y_test, y_pred_test))\n",
        "print(\"Confusion matrix (test):\\n\", confusion_matrix(y_test, y_pred_test))\n",
        "print(\"Class report (test):\\n\", classification_report(y_test, y_pred_test))\n",
        "\n",
        "T = 15\n",
        "n_train = X_train.shape[0]\n",
        "w = np.ones(n_train) / n_train\n",
        "models = []\n",
        "alphas = []\n",
        "weighted_errors = []\n",
        "\n",
        "for t in range(1, T+1):\n",
        "    stump_t = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "    stump_t.fit(X_train, y_train, sample_weight=w)\n",
        "    pred = stump_t.predict(X_train)\n",
        "\n",
        "    miss = (pred != y_train).astype(int)\n",
        "\n",
        "    eps = np.sum(w * miss) / np.sum(w)\n",
        "\n",
        "    eps = np.clip(eps, 1e-12, 1-1e-12)\n",
        "    alpha = 0.5 * np.log((1 - eps) / eps)\n",
        "\n",
        "    y_signed = np.where(y_train==1, 1, -1)\n",
        "    pred_signed = np.where(pred==1, 1, -1)\n",
        "    w = w * np.exp(-alpha * y_signed * pred_signed)\n",
        "    w = w / np.sum(w)\n",
        "    models.append(stump_t); alphas.append(alpha); weighted_errors.append(eps)\n",
        "\n",
        "    mis_idx = np.where(miss==1)[0]\n",
        "    print(f\"Iter {t} â€” eps={eps:.5f}, alpha={alpha:.5f}, #mis={len(mis_idx)}\")\n",
        "    print(\"First 10 misclassified train indices:\", mis_idx[:10])\n",
        "    print(\"Weights of first 10 misclassified examples:\", w[mis_idx[:10]])\n",
        "    print(\"----\")\n",
        "\n",
        "def ada_predict(models, alphas, X):\n",
        "\n",
        "    agg = None\n",
        "    for m,a in zip(models, alphas):\n",
        "        p = m.predict(X)\n",
        "        p_signed = np.where(p==1, 1, -1)\n",
        "        if agg is None:\n",
        "            agg = a * p_signed\n",
        "        else:\n",
        "            agg += a * p_signed\n",
        "    return (np.sign(agg) > 0).astype(int)\n",
        "\n",
        "y_train_pred = ada_predict(models, alphas, X_train)\n",
        "y_test_pred = ada_predict(models, alphas, X_test)\n",
        "print(\"Manual Ada: Train Acc:\", accuracy_score(y_train, y_train_pred))\n",
        "print(\"Manual Ada: Test  Acc:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"Confusion matrix (test):\\n\", confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1); plt.plot(range(1,T+1), weighted_errors, marker='o'); plt.title(\"Iteration vs Weighted error\")\n",
        "plt.subplot(1,2,2); plt.plot(range(1,T+1), alphas, marker='o'); plt.title(\"Iteration vs Alpha\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "adb = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.6, random_state=42)\n",
        "adb.fit(X_train, y_train)\n",
        "y_train_adb = adb.predict(X_train); y_test_adb = adb.predict(X_test)\n",
        "print(\"Sklearn AdaBoost Train Acc:\", accuracy_score(y_train, y_train_adb))\n",
        "print(\"Sklearn AdaBoost Test  Acc:\", accuracy_score(y_test, y_test_adb))\n",
        "print(\"Confusion matrix (test):\\n\", confusion_matrix(y_test, y_test_adb))\n"
      ],
      "metadata": {
        "id": "jfkEbCSbu3fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "try:\n",
        "    heart = fetch_openml(name='heart', version=1, as_frame=True)\n",
        "    X = heart.data\n",
        "    y = heart.target.astype(int)\n",
        "    print(\"Loaded from OpenML:\", heart.DESCR[:200])\n",
        "except Exception as e:\n",
        "    print(\"OpenML load failed:\", e)\n",
        "\n",
        "X = X.copy()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == object:\n",
        "\n",
        "        X[col] = pd.to_numeric(X[col], errors='ignore')\n",
        "\n",
        "\n",
        "categorical_cols = [c for c in X.columns if X[c].nunique() <= 6 and X[c].dtype in [np.int64, np.int32, np.object_]]\n",
        "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
        "\n",
        "print(\"Numeric cols:\", numeric_cols)\n",
        "print(\"Categorical cols:\", categorical_cols)\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), numeric_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "stump_pipe = Pipeline([('pre', preprocessor), ('clf', DecisionTreeClassifier(max_depth=1, random_state=42))])\n",
        "stump_pipe.fit(X_train, y_train)\n",
        "y_tr_pred = stump_pipe.predict(X_train); y_te_pred = stump_pipe.predict(X_test)\n",
        "print(\"Stump Train Acc:\", accuracy_score(y_train, y_tr_pred))\n",
        "print(\"Stump Test  Acc:\", accuracy_score(y_test, y_te_pred))\n",
        "print(\"Confusion matrix (test):\\n\", confusion_matrix(y_test, y_te_pred))\n",
        "print(\"Classification report (test):\\n\", classification_report(y_test, y_te_pred))\n",
        "\n",
        "n_estimators_list = [5,10,25,50,100]\n",
        "learning_rates = [0.1, 0.5, 1.0]\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    accs = []\n",
        "    for n in n_estimators_list:\n",
        "        adb_pipe = Pipeline([('pre', preprocessor),\n",
        "                             ('clf', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                                                        n_estimators=n, learning_rate=lr, random_state=42))])\n",
        "        adb_pipe.fit(X_train, y_train)\n",
        "        y_pred = adb_pipe.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        accs.append(acc)\n",
        "        results.append({'lr': lr, 'n': n, 'acc': acc})\n",
        "    plt.plot(n_estimators_list, accs, marker='o', label=f\"lr={lr}\")\n",
        "\n",
        "plt.xlabel(\"n_estimators\"); plt.ylabel(\"Test accuracy\"); plt.legend(); plt.title(\"AdaBoost: n_estimators vs accuracy\"); plt.grid(True); plt.show()\n",
        "\n",
        "res_df = pd.DataFrame(results)\n",
        "best_row = res_df.loc[res_df['acc'].idxmax()]\n",
        "print(\"Best config:\", best_row.to_dict())\n",
        "\n",
        "preproc = preprocessor.fit(X_train)\n",
        "X_train_proc = preproc.transform(X_train)\n",
        "X_test_proc  = preproc.transform(X_test)\n",
        "\n",
        "def manual_adaboost_track(X, y, T=50):\n",
        "    n = X.shape[0]\n",
        "    w = np.ones(n)/n\n",
        "    models, alphas, errors = [], [], []\n",
        "    for t in range(T):\n",
        "        stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "        stump.fit(X, y, sample_weight=w)\n",
        "        pred = stump.predict(X)\n",
        "        miss = (pred != y).astype(int)\n",
        "        eps = np.sum(w * miss)\n",
        "        eps = np.clip(eps, 1e-12, 1-1e-12)\n",
        "        alpha = 0.5 * np.log((1 - eps)/eps)\n",
        "\n",
        "        y_signed = np.where(y==1, 1, -1)\n",
        "        pred_signed = np.where(pred==1, 1, -1)\n",
        "        w = w * np.exp(-alpha * y_signed * pred_signed)\n",
        "        w = w / np.sum(w)\n",
        "        models.append(stump); alphas.append(alpha); errors.append(eps)\n",
        "    return models, alphas, errors, w\n",
        "\n",
        "T_best = int(best_row['n'])\n",
        "models_b, alphas_b, errors_b, final_weights = manual_adaboost_track(X_train_proc, y_train.values, T=T_best)\n",
        "\n",
        "plt.figure(); plt.plot(range(1,T_best+1), errors_b, marker='o'); plt.xlabel('Iteration'); plt.ylabel('Weak learner error'); plt.title('Weak learner error vs iteration'); plt.grid(True)\n",
        "\n",
        "plt.figure(); plt.hist(final_weights, bins=30); plt.title('Final sample weight distribution'); plt.show()\n",
        "\n",
        "top_idx = np.argsort(final_weights)[-10:][::-1]\n",
        "print(\"Top weighted training samples (indices):\", top_idx)\n",
        "print(\"Their labels:\", y_train.values[top_idx])\n",
        "\n",
        "adb_best = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=T_best, learning_rate=best_row['lr'], random_state=42)\n",
        "adb_best.fit(X_train_proc, y_train)\n",
        "feat_importances = adb_best.feature_importances_\n",
        "\n",
        "ohe = preproc.named_transformers_['cat']\n",
        "ohe_cat_names = []\n",
        "if hasattr(ohe, 'get_feature_names_out'):\n",
        "    ohe_cat_names = list(preproc.transformers_[1][1].get_feature_names_out(categorical_cols))\n",
        "else:\n",
        "    try:\n",
        "        ohe_cat_names = list(ohe.get_feature_names(categorical_cols))\n",
        "    except:\n",
        "        ohe_cat_names = [\"cat_{}\".format(i) for i in range(sum(len(vals.categories_[i]) for i in range(len(categorical_cols))))]\n",
        "num_names = numeric_cols\n",
        "feature_names = list(num_names) + ohe_cat_names\n",
        "# Top 5 features\n",
        "top5_idx = np.argsort(feat_importances)[-5:][::-1]\n",
        "print(\"Top 5 important features and importances:\")\n",
        "for i in top5_idx:\n",
        "    print(feature_names[i], feat_importances[i])\n"
      ],
      "metadata": {
        "id": "02FMfpHLv8zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "path = \"WISDM_ar_v1.1_raw.txt\"\n",
        "rows = []\n",
        "with open(path, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        parts = re.split(r'[,\\s]+', line)\n",
        "\n",
        "        if len(parts) < 6:\n",
        "            continue\n",
        "        user, activity, timestamp, x, y, z = parts[0], parts[1], parts[2], parts[3], parts[4], parts[5]\n",
        "        try:\n",
        "            rows.append([int(user), activity, int(timestamp), float(x), float(y), float(z)])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "df = pd.DataFrame(rows, columns=['user','activity','timestamp','x','y','z'])\n",
        "\n",
        "vigorous = {'Jogging','Upstairs','Jogging,','Upstairs,'}\n",
        "df['activity_norm'] = df['activity'].str.strip().str.lower()\n",
        "\n",
        "df['label'] = df['activity_norm'].apply(lambda s: 1 if ('jog' in s) or ('up' in s) else 0)\n",
        "\n",
        "df = df.dropna(subset=['x','y','z'])\n",
        "\n",
        "X = df[['x','y','z']].values\n",
        "y = df['label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "stump.fit(X_train, y_train)\n",
        "print(\"Stump train acc\", accuracy_score(y_train, stump.predict(X_train)))\n",
        "print(\"Stump test  acc\", accuracy_score(y_test, stump.predict(X_test)))\n",
        "print(\"Confusion (test):\\n\", confusion_matrix(y_test, stump.predict(X_test)))\n",
        "\n",
        "def ada_manual(X_tr, y_tr, X_te, T=20):\n",
        "    n = X_tr.shape[0]\n",
        "    w = np.ones(n)/n\n",
        "    models, alphas = [], []\n",
        "    for t in range(T):\n",
        "        stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "        stump.fit(X_tr, y_tr, sample_weight=w)\n",
        "        pred = stump.predict(X_tr)\n",
        "        miss = (pred != y_tr).astype(int)\n",
        "        eps = np.sum(w * miss)\n",
        "        eps = np.clip(eps, 1e-12, 1-1e-12)\n",
        "        alpha = 0.5*np.log((1-eps)/eps)\n",
        "        y_signed = np.where(y_tr==1, 1, -1); pred_signed = np.where(pred==1,1,-1)\n",
        "        w = w * np.exp(-alpha*y_signed*pred_signed); w = w/np.sum(w)\n",
        "        models.append(stump); alphas.append(alpha)\n",
        "\n",
        "        print(f\"Iter {t+1}: eps={eps:.4f}, alpha={alpha:.4f}, #mis={(miss==1).sum()}\")\n",
        "\n",
        "    def predict_combo(X):\n",
        "        agg = np.zeros(X.shape[0])\n",
        "        for m,a in zip(models, alphas):\n",
        "            agg += a * (2*m.predict(X)-1)\n",
        "        return (np.sign(agg)>0).astype(int)\n",
        "    return models, alphas, predict_combo\n",
        "\n",
        "models_a, alphas_a, predict_fun = ada_manual(X_train, y_train, X_test, T=20)\n",
        "y_train_pred = predict_fun(X_train); y_test_pred = predict_fun(X_test)\n",
        "print(\"Manual Ada: train acc\", accuracy_score(y_train, y_train_pred))\n",
        "print(\"Manual Ada: test  acc\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"Confusion test:\\n\", confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "adb = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=1.0, random_state=42)\n",
        "adb.fit(X_train, y_train)\n",
        "print(\"Sklearn Ada train acc\", accuracy_score(y_train, adb.predict(X_train)))\n",
        "print(\"Sklearn Ada test acc\", accuracy_score(y_test, adb.predict(X_test)))\n",
        "print(\"Confusion test:\\n\", confusion_matrix(y_test, adb.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "PUTV4F4-wssm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}