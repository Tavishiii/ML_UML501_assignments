{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "QUES1"
      ],
      "metadata": {
        "id": "rNor0wLYY5pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Step 1: Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "base_feature = np.random.rand(n_samples)\n",
        "\n",
        "# Create 7 highly correlated features\n",
        "n_features = 7\n",
        "features = [base_feature + np.random.randn(n_samples) * 0.05 for _ in range(n_features)]\n",
        "X = np.column_stack(features)\n",
        "\n",
        "# True coefficients\n",
        "true_theta = np.array([10, 9, 8, 7, 6, 5, 4])\n",
        "y = X.dot(true_theta) + np.random.randn(n_samples) * 0.5  # add noise\n",
        "\n",
        "# Step 2: Normalize features\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "m, n = X.shape\n",
        "\n",
        "# Step 3: Add bias term (column of ones)\n",
        "X = np.c_[np.ones((m, 1)), X]  # shape (m, n+1)\n",
        "\n",
        "# Step 4: Ridge Cost Function\n",
        "def ridge_cost(X, y, theta, lam):\n",
        "    m = len(y)\n",
        "    predictions = X.dot(theta)\n",
        "    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2) + (lam / (2 * m)) * np.sum(theta[1:] ** 2)\n",
        "    return cost\n",
        "\n",
        "# Step 5: Gradient Descent Update\n",
        "def ridge_gradient_descent(X, y, theta, alpha, lam, num_iters):\n",
        "    m = len(y)\n",
        "    cost_history = []\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        predictions = X.dot(theta)\n",
        "        gradient = (1 / m) * (X.T.dot(predictions - y)) + (lam / m) * np.r_[[0], theta[1:]]\n",
        "        theta -= alpha * gradient\n",
        "        cost_history.append(ridge_cost(X, y, theta, lam))\n",
        "        # Added check for NaN in theta to prevent further calculations with NaNs\n",
        "        if np.isnan(theta).any():\n",
        "            print(f\"Warning: NaN encountered in theta at iteration {_} for alpha={alpha}, lambda={lam}. Stopping gradient descent.\")\n",
        "            break\n",
        "\n",
        "\n",
        "    return theta, cost_history\n",
        "\n",
        "# Step 6: Try different learning rates and lambda values\n",
        "# Reduced learning rates to avoid divergence\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "lambdas = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
        "\n",
        "best_score = -np.inf\n",
        "best_params = {}\n",
        "best_theta = None\n",
        "\n",
        "print(\"Starting hyperparameter tuning with reduced learning rates...\")\n",
        "\n",
        "for alpha in learning_rates:\n",
        "    for lam in lambdas:\n",
        "        print(f\"Testing learning_rate: {alpha}, lambda: {lam}\")\n",
        "        theta = np.zeros(n + 1)\n",
        "        theta, cost_history = ridge_gradient_descent(X, y, theta, alpha, lam, num_iters=1000)\n",
        "\n",
        "        # Check if theta contains NaN before calculating predictions and R2 score\n",
        "        if np.isnan(theta).any():\n",
        "            print(f\"Skipping evaluation for alpha={alpha}, lambda={lam} due to NaN in theta.\")\n",
        "            continue\n",
        "\n",
        "        y_pred = X.dot(theta)\n",
        "\n",
        "        # Added check for NaN in y_pred before calculating R2 score\n",
        "        if np.isnan(y_pred).any():\n",
        "             print(f\"Skipping R2 calculation for alpha={alpha}, lambda={lam} due to NaN in predictions.\")\n",
        "             continue\n",
        "\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        final_cost = ridge_cost(X, y, theta, lam)\n",
        "\n",
        "        if r2 > best_score:\n",
        "            best_score = r2\n",
        "            best_params = {'alpha': alpha, 'lambda': lam, 'cost': final_cost}\n",
        "            best_theta = theta\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete.\")\n",
        "print(\"✅ Best parameters found:\")\n",
        "print(best_params)\n",
        "print(\"Best R² score:\", round(best_score, 4))\n",
        "\n",
        "# Step 7: Show final coefficients\n",
        "print(\"\\nFinal coefficients (theta):\\n\", best_theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-IedviQY67M",
        "outputId": "0deada42-c29b-4a73-a94c-196991d6b53b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting hyperparameter tuning with reduced learning rates...\n",
            "Testing learning_rate: 0.0001, lambda: 1e-15\n",
            "Testing learning_rate: 0.0001, lambda: 1e-10\n",
            "Testing learning_rate: 0.0001, lambda: 1e-05\n",
            "Testing learning_rate: 0.0001, lambda: 0.001\n",
            "Testing learning_rate: 0.0001, lambda: 0\n",
            "Testing learning_rate: 0.0001, lambda: 1\n",
            "Testing learning_rate: 0.0001, lambda: 10\n",
            "Testing learning_rate: 0.0001, lambda: 20\n",
            "Testing learning_rate: 0.001, lambda: 1e-15\n",
            "Testing learning_rate: 0.001, lambda: 1e-10\n",
            "Testing learning_rate: 0.001, lambda: 1e-05\n",
            "Testing learning_rate: 0.001, lambda: 0.001\n",
            "Testing learning_rate: 0.001, lambda: 0\n",
            "Testing learning_rate: 0.001, lambda: 1\n",
            "Testing learning_rate: 0.001, lambda: 10\n",
            "Testing learning_rate: 0.001, lambda: 20\n",
            "Testing learning_rate: 0.01, lambda: 1e-15\n",
            "Testing learning_rate: 0.01, lambda: 1e-10\n",
            "Testing learning_rate: 0.01, lambda: 1e-05\n",
            "Testing learning_rate: 0.01, lambda: 0.001\n",
            "Testing learning_rate: 0.01, lambda: 0\n",
            "Testing learning_rate: 0.01, lambda: 1\n",
            "Testing learning_rate: 0.01, lambda: 10\n",
            "Testing learning_rate: 0.01, lambda: 20\n",
            "Testing learning_rate: 0.1, lambda: 1e-15\n",
            "Testing learning_rate: 0.1, lambda: 1e-10\n",
            "Testing learning_rate: 0.1, lambda: 1e-05\n",
            "Testing learning_rate: 0.1, lambda: 0.001\n",
            "Testing learning_rate: 0.1, lambda: 0\n",
            "Testing learning_rate: 0.1, lambda: 1\n",
            "Testing learning_rate: 0.1, lambda: 10\n",
            "Testing learning_rate: 0.1, lambda: 20\n",
            "\n",
            "Hyperparameter tuning complete.\n",
            "✅ Best parameters found:\n",
            "{'alpha': 0.1, 'lambda': 1e-15, 'cost': np.float64(0.13333377902779625)}\n",
            "Best R² score: 0.9987\n",
            "\n",
            "Final coefficients (theta):\n",
            " [24.05375578  2.91794978  2.6773877   2.34490391  2.00975412  1.79312711\n",
            "  1.52081108  1.23339356]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES2"
      ],
      "metadata": {
        "id": "7chrCfKIWqIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# ✅ Step 1: Load dataset from GitHub (works directly)\n",
        "url = \"https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Hitters.csv\"\n",
        "hitters = pd.read_csv(url)\n",
        "\n",
        "print(\"Dataset loaded successfully ✅\")\n",
        "print(\"Shape:\", hitters.shape)\n",
        "print(\"\\nFirst few rows:\\n\", hitters.head())\n",
        "\n",
        "# Step 2: Data preprocessing\n",
        "print(\"\\nMissing values before cleaning:\\n\", hitters.isnull().sum())\n",
        "\n",
        "# Drop rows with null Salary\n",
        "hitters.dropna(inplace=True)\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "hitters_encoded = pd.get_dummies(hitters, drop_first=True)\n",
        "\n",
        "print(\"\\nAfter encoding shape:\", hitters_encoded.shape)\n",
        "\n",
        "# Step 3: Split into input and output\n",
        "X = hitters_encoded.drop(\"Salary\", axis=1)\n",
        "y = hitters_encoded[\"Salary\"]\n",
        "\n",
        "# Step 4: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 6: Train models\n",
        "lin_reg = LinearRegression().fit(X_train_scaled, y_train)\n",
        "ridge_reg = Ridge(alpha=0.5748).fit(X_train_scaled, y_train)\n",
        "lasso_reg = Lasso(alpha=0.5748).fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 7: Evaluate\n",
        "models = {\n",
        "    \"Linear Regression\": lin_reg,\n",
        "    \"Ridge Regression\": ridge_reg,\n",
        "    \"Lasso Regression\": lasso_reg\n",
        "}\n",
        "\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "# Step 8: Model comparison summary\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"- Ridge Regression usually performs best because it reduces overfitting using L2 regularization.\")\n",
        "print(\"- Lasso can eliminate less important features, improving interpretability.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXrkqGNGX7Xg",
        "outputId": "0b15ed1a-597c-43da-9768-5e799b8cd17f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully ✅\n",
            "Shape: (322, 21)\n",
            "\n",
            "First few rows:\n",
            "           Unnamed: 0  AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  \\\n",
            "0     -Andy Allanson    293    66      1    30   29     14      1     293   \n",
            "1        -Alan Ashby    315    81      7    24   38     39     14    3449   \n",
            "2       -Alvin Davis    479   130     18    66   72     76      3    1624   \n",
            "3      -Andre Dawson    496   141     20    65   78     37     11    5628   \n",
            "4  -Andres Galarraga    321    87     10    39   42     30      2     396   \n",
            "\n",
            "   CHits  ...  CRuns  CRBI  CWalks  League Division PutOuts  Assists  Errors  \\\n",
            "0     66  ...     30    29      14       A        E     446       33      20   \n",
            "1    835  ...    321   414     375       N        W     632       43      10   \n",
            "2    457  ...    224   266     263       A        W     880       82      14   \n",
            "3   1575  ...    828   838     354       N        E     200       11       3   \n",
            "4    101  ...     48    46      33       N        E     805       40       4   \n",
            "\n",
            "   Salary  NewLeague  \n",
            "0     NaN          A  \n",
            "1   475.0          N  \n",
            "2   480.0          A  \n",
            "3   500.0          N  \n",
            "4    91.5          N  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "\n",
            "Missing values before cleaning:\n",
            " Unnamed: 0     0\n",
            "AtBat          0\n",
            "Hits           0\n",
            "HmRun          0\n",
            "Runs           0\n",
            "RBI            0\n",
            "Walks          0\n",
            "Years          0\n",
            "CAtBat         0\n",
            "CHits          0\n",
            "CHmRun         0\n",
            "CRuns          0\n",
            "CRBI           0\n",
            "CWalks         0\n",
            "League         0\n",
            "Division       0\n",
            "PutOuts        0\n",
            "Assists        0\n",
            "Errors         0\n",
            "Salary        59\n",
            "NewLeague      0\n",
            "dtype: int64\n",
            "\n",
            "After encoding shape: (263, 282)\n",
            "\n",
            "--- Model Evaluation ---\n",
            "\n",
            "Linear Regression\n",
            "Mean Squared Error: 150540.93\n",
            "R² Score: 0.1677\n",
            "\n",
            "Ridge Regression\n",
            "Mean Squared Error: 150168.56\n",
            "R² Score: 0.1698\n",
            "\n",
            "Lasso Regression\n",
            "Mean Squared Error: 154316.19\n",
            "R² Score: 0.1468\n",
            "\n",
            "Conclusion:\n",
            "- Ridge Regression usually performs best because it reduces overfitting using L2 regularization.\n",
            "- Lasso can eliminate less important features, improving interpretability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES3"
      ],
      "metadata": {
        "id": "oTiE_JNVYGGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "print(\"Dataset loaded successfully ✅\")\n",
        "print(\"Shape:\", X.shape)\n",
        "print(\"Features:\", list(X.columns))\n",
        "\n",
        "# Step 2: Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Standardize (important for regularized regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: RidgeCV - automatically selects the best alpha\n",
        "ridge_alphas = [0.1, 1, 10, 100]\n",
        "ridge_cv = RidgeCV(alphas=ridge_alphas, cv=5)\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: LassoCV - automatically selects the best alpha\n",
        "lasso_cv = LassoCV(alphas=np.logspace(-3, 3, 50), cv=5, random_state=42)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 6: Evaluate both\n",
        "models = {\n",
        "    \"RidgeCV\": ridge_cv,\n",
        "    \"LassoCV\": lasso_cv\n",
        "}\n",
        "\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(f\"Best Alpha: {model.alpha_:.4f}\")\n",
        "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "# Step 7: Compare models\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"- RidgeCV usually performs slightly better on correlated features.\")\n",
        "print(\"- LassoCV may set some coefficients to zero → feature selection.\")\n",
        "print(\"- The chosen alpha shows how much regularization the model needs.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1iwXzsUYHLn",
        "outputId": "adc9c929-3562-4dc5-c381-1f41f4b7ffd0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully ✅\n",
            "Shape: (20640, 8)\n",
            "Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
            "\n",
            "--- Model Evaluation ---\n",
            "\n",
            "RidgeCV\n",
            "Best Alpha: 0.1000\n",
            "Mean Squared Error: 0.5559\n",
            "R² Score: 0.5758\n",
            "\n",
            "LassoCV\n",
            "Best Alpha: 0.0010\n",
            "Mean Squared Error: 0.5545\n",
            "R² Score: 0.5769\n",
            "\n",
            "Conclusion:\n",
            "- RidgeCV usually performs slightly better on correlated features.\n",
            "- LassoCV may set some coefficients to zero → feature selection.\n",
            "- The chosen alpha shows how much regularization the model needs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES4"
      ],
      "metadata": {
        "id": "Ztc512DNYVu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "\n",
        "# Step 2: Split into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Feature scaling (important for gradient-based algorithms)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Create Logistic Regression model\n",
        "# 'ovr' = One-vs-Rest, 'multinomial' = Softmax\n",
        "log_reg = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Predictions\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nModel Accuracy:\", round(accuracy, 3))\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=class_names))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kVttHXgYWx7",
        "outputId": "3367bfe2-5aaa-4d84-8cd9-592e3721432c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['setosa' 'versicolor' 'virginica']\n",
            "Shape of X: (150, 4)\n",
            "Shape of y: (150,)\n",
            "\n",
            "Model Accuracy: 0.9\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       0.89      0.80      0.84        10\n",
            "   virginica       0.82      0.90      0.86        10\n",
            "\n",
            "    accuracy                           0.90        30\n",
            "   macro avg       0.90      0.90      0.90        30\n",
            "weighted avg       0.90      0.90      0.90        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  8  2]\n",
            " [ 0  1  9]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}